{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "770837a2",
   "metadata": {},
   "source": [
    "# Steps to EQ Rotta\n",
    "\n",
    "*Authors: Antony Sikorski, Sara Shamekh*\n",
    "\n",
    "My modifications to the original EQ-Rotta notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3489d55e-59c3-4129-be0a-6c5923186507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pdfs import *\n",
    "import os\n",
    "\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "import h5netcdf\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f98f68",
   "metadata": {},
   "source": [
    "Should return all files and folders that start with 'Ug', although in our case right now it should just be files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3f5ec4-060f-46c6-8676-5553505315ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_directories_files(path):\n",
    "    # all items in the given path\n",
    "    items = os.listdir(path)\n",
    "    \n",
    "    # find the directories starting with 'Ug'\n",
    "    directories = [item for item in items if os.path.isdir(os.path.join(path, item)) and item.startswith('Ug')]\n",
    "    # find the files starting with 'Ug'\n",
    "    files = [item for item in items if os.path.isfile(os.path.join(path, item)) and item.startswith('Ug')]\n",
    "    \n",
    "    return directories, files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ddaf8",
   "metadata": {},
   "source": [
    "This lists all of our files. **All that needs to be changed is the path.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd722e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/anton/Desktop/Career/LEAP_nyc_Summer2024/les_sim_2/'\n",
    "\n",
    "\n",
    "directories, files = list_directories_files(path)\n",
    "print(\"Directories starting with 'Ug':\", directories)\n",
    "print(\"Files starting with 'Ug':\", files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cbcb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nc_test1 = nc.Dataset(os.path.join('C:/Users/anton/Desktop/Career/LEAP_nyc_Summer2024/les_sim_2/', 'Ug2Q010_I.nc'), mode='r')\n",
    "\n",
    "# z_256 = nc_test1.variables['z'][:]\n",
    "# zh_257 = nc_test1.variables['zh'][:]\n",
    "# t_120 = nc_test1.variables['time'][:]\n",
    "\n",
    "# #rewrite this code but using 'Ug2Q010_IV.nc' instead\n",
    "# nc_test2 = nc.Dataset(os.path.join('C:/Users/anton/Desktop/Career/LEAP_nyc_Summer2024/les_sim_2/', 'Ug2Q010_IV.nc'), mode='r')\n",
    "\n",
    "# z_384 = nc_test2.variables['z'][:]\n",
    "# zh_385 = nc_test2.variables['zh'][:]\n",
    "# t_90 = nc_test2.variables['time'][:]\n",
    "\n",
    "# # print(len(z_256), len(zh_257), len(t_120), len(z_384), len(zh_385), len(t_90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ffc386-7bd0-4b80-a561-4a21edd093a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_average(data, timeavg):\n",
    "    \"\"\"\n",
    "    Averages an array over specified number of time steps. Works for both 1D and higher-dimensional arrays.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): The input data array. Expected shapes are either (ntime,) or (ntime, nz).\n",
    "    - timeavg (int): The number of time steps over which to average.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The averaged data array.\n",
    "    \"\"\"\n",
    "    ntime = data.shape[0]\n",
    "    nchunks = ntime // timeavg\n",
    "    truncated_data = data[:nchunks * timeavg]\n",
    "\n",
    "    if data.ndim == 1:\n",
    "        # for 1D arrays, reshape to (nchunks, timeavg)\n",
    "        reshaped_data = truncated_data.reshape(nchunks, timeavg)\n",
    "    else:\n",
    "        # for 2D arrays, old method with reshaping\n",
    "        nz = data.shape[1]\n",
    "        reshaped_data = truncated_data.reshape(nchunks, timeavg, nz)\n",
    "\n",
    "    # compute the mean along the new time axis \n",
    "    averaged_data = reshaped_data.mean(axis=1)\n",
    "\n",
    "    return averaged_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddaec8f-52de-4632-9cfa-237ebb5ace53",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = files\n",
    "\n",
    "time_avg = 15\n",
    "\n",
    "for item in items:\n",
    "    ds_stat = nc.Dataset( os.path.join(path, item), mode='r')\n",
    "    if 'budget' in ds_stat.groups:\n",
    "        print (\"budget is in\", item)\n",
    "    else:\n",
    "        print (\"budget is not in\", item)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7966436",
   "metadata": {},
   "source": [
    "It appears that we can use all files except the very first (000) file. Going to print out all of the viable files so we can see dimension sizes and groups: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255a09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in items[1:]:\n",
    "    print(item)\n",
    "    df = nc.Dataset(path + '/' + item, mode='r')\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8073b60",
   "metadata": {},
   "source": [
    "This function should create variables for a given list of `items`. In my case, my `selected_files` constitutes a group of similar (I think) simulations that I can append together for the sake of having more data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a31a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_files = [items[1], items[2], items[4], items[5]]\n",
    "# selected_files = [items[1]]\n",
    "#selected_files = [items[1], items[2], items[4], items[5], items[6], items[8],items[9], items[11], items[12], items[13], items[14]]\n",
    "\n",
    "target_z_dim = 384\n",
    "\n",
    "selected_files = []\n",
    "\n",
    "for item in items[1:]:\n",
    "        df = nc.Dataset(os.path.join(path, item), mode='r')\n",
    "        if df.dimensions['z'].size == target_z_dim:\n",
    "            selected_files.append(item)\n",
    "        df.close()\n",
    "\n",
    "print(len(selected_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c006e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = nc.Dataset(os.path.join(path, selected_files[0]), mode='r')   \n",
    "\n",
    "for group_name, group in df.groups.items():\n",
    "    print(f\"Group: {group_name}\")\n",
    "    for var_name, var in group.variables.items():\n",
    "        long_name = var.getncattr('long_name') if 'long_name' in var.ncattrs() else 'No long_name attribute'\n",
    "        print(f\"  Variable: {var_name}, Long Name: {long_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352d12f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_variables(path, items, time_avg):\n",
    "    # Initialize empty lists to accumulate results\n",
    "    sigma_th = []\n",
    "    sigma_2 = []\n",
    "    Theta = []\n",
    "    wtheta = []\n",
    "    wwtheta = []\n",
    "    rdstr = []\n",
    "    transport = []\n",
    "\n",
    "    for item in items:\n",
    "        ds_stat = nc.Dataset(os.path.join(path, item), mode='r')\n",
    "        \n",
    "        sigma_th_temp = time_average(ds_stat.groups['thermo']['th_2'][:], time_avg)  # covariance of theta\n",
    "        sigma_2_temp = time_average(ds_stat.groups['default']['w_2'][:], time_avg)   # covariance of w\n",
    "        Theta_temp = time_average(ds_stat.groups['thermo']['th'][:], time_avg)       # domain mean theta\n",
    "        wtheta_temp = time_average(ds_stat.groups['thermo']['th_flux'][:], time_avg) # heat flux\n",
    "        wwtheta_temp = time_average(ds_stat.groups['budget']['wwtheta'][:], time_avg) # third moment, covariance between wtheta and w\n",
    "        rdstr_temp = time_average(ds_stat.groups['budget']['bw_rdstr'][:], time_avg)\n",
    "        transport_temp = time_average(ds_stat.groups['budget']['bw_pres'][:], time_avg)\n",
    "\n",
    "        # Append the results to the respective lists\n",
    "        sigma_th.append(sigma_th_temp)\n",
    "        sigma_2.append(sigma_2_temp)\n",
    "        Theta.append(Theta_temp)\n",
    "        wtheta.append(wtheta_temp)\n",
    "        wwtheta.append(wwtheta_temp)\n",
    "        rdstr.append(rdstr_temp)\n",
    "        transport.append(transport_temp)\n",
    "\n",
    "    # Concatenate the results along the time axis\n",
    "    sigma_th = np.concatenate(sigma_th, axis=0)\n",
    "    sigma_2 = np.concatenate(sigma_2, axis=0)\n",
    "    Theta = np.concatenate(Theta, axis=0)\n",
    "    wtheta = np.concatenate(wtheta, axis=0)\n",
    "    wwtheta = np.concatenate(wwtheta, axis=0)\n",
    "    rdstr = np.concatenate(rdstr, axis=0)\n",
    "    transport = np.concatenate(transport, axis=0)\n",
    "\n",
    "    return sigma_th, sigma_2, Theta, wtheta, wwtheta, rdstr, transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1346672",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_th, sigma_2, Theta, wtheta, wwtheta, rdstr, transport = make_variables(path, selected_files, time_avg)\n",
    "\n",
    "# z = np.arange(0, 384)\n",
    "# zh = np.arange(0, 385)\n",
    "# t = np.arange(0, 120)\n",
    "\n",
    "z = ds_stat.variables['z'][:]\n",
    "zh = ds_stat.variables['zh'][:]\n",
    "t = ds_stat.variables['time'][:]\n",
    "\n",
    "\n",
    "dTheta_dz = np.gradient(Theta, z, axis = 1)\n",
    "dwwtheta_dz = np.gradient(wwtheta, zh, axis = 1)\n",
    "\n",
    "print(zh.shape, z.shape, t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c4eea2",
   "metadata": {},
   "source": [
    "Let's take a look at all of the variables (including gradients) that we create. We print their:\n",
    "\n",
    "Name, shape, mean, and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15890a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dTheta_dz\", dTheta_dz.shape, np.mean(dTheta_dz), np.std(dTheta_dz))\n",
    "print(\"sigma_th\", sigma_th.shape, np.mean(sigma_th), np.std(sigma_th))\n",
    "print(\"sigma_2\", sigma_2.shape, np.mean(sigma_2), np.std(sigma_2))\n",
    "print(\"Theta\", Theta.shape, np.mean(Theta), np.std(Theta))\n",
    "print(\"wtheta\", wtheta.shape, np.mean(wtheta), np.std(wtheta))\n",
    "print(\"wwtheta\", wwtheta.shape, np.mean(wwtheta), np.std(wwtheta))\n",
    "print(\"dwwtheta_dz\", dwwtheta_dz.shape, np.mean(dwwtheta_dz), np.std(dwwtheta_dz))\n",
    "print(\"rdtsr\", rdstr.shape, np.mean(rdstr), np.std(rdstr))\n",
    "print(\"transport\", transport.shape, np.mean(transport), np.std(transport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7349d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wwtheta[1,0:3])\n",
    "print((wwtheta[1,0]+wwtheta[1,1])/2, (wwtheta[1,1]+wwtheta[1,2])/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc1956f",
   "metadata": {},
   "source": [
    "Variables that have a second dimension of 385 are calculated at zh values of 0, 6, 12, and so on. Variables with 384 are staggered and calculated in between there, at values of 3, 9, 15, and so on. We reshape the 385's to be 384's by taking the average of each consecutive pair. \n",
    "\n",
    "We test if this works above and below with old and new values of `wwtheta` (just a sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e1977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_variables(variable):\n",
    "    reshaped = (variable[:, :-1] + variable[:, 1:]) / 2.0\n",
    "    return reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f926192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_2 = reshape_variables(sigma_2)\n",
    "wtheta = reshape_variables(wtheta)\n",
    "wwtheta = reshape_variables(wwtheta)\n",
    "dwwtheta_dz = reshape_variables(dwwtheta_dz)\n",
    "rdstr = reshape_variables(rdstr)\n",
    "transport = reshape_variables(transport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71224502",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dTheta_dz\", dTheta_dz.shape, np.mean(dTheta_dz), np.std(dTheta_dz))\n",
    "print(\"sigma_th\", sigma_th.shape, np.mean(sigma_th), np.std(sigma_th))\n",
    "print(\"sigma_2\", sigma_2.shape, np.mean(sigma_2), np.std(sigma_2))\n",
    "print(\"Theta\", Theta.shape, np.mean(Theta), np.std(Theta))\n",
    "print(\"wtheta\", wtheta.shape, np.mean(wtheta), np.std(wtheta))\n",
    "print(\"wwtheta\", wwtheta.shape, np.mean(wwtheta), np.std(wwtheta))\n",
    "print(\"dwwtheta_dz\", dwwtheta_dz.shape, np.mean(dwwtheta_dz), np.std(dwwtheta_dz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c540d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wwtheta[1,0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd68b161",
   "metadata": {},
   "source": [
    "Plotting all variables at a given timestep just for a sanity check: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b8ec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeStep = 6\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "\n",
    "# Plot potentialTemp\n",
    "plt.subplot(1, 3, 1)\n",
    "#plt.plot(Theta[timeStep], np.arange(0, len(Theta[timeStep,:])))\n",
    "plt.plot(Theta[timeStep], z)\n",
    "plt.title('Theta(th)')\n",
    "# plt.xlabel('Value')\n",
    "plt.ylabel('Depth')\n",
    "\n",
    "# Plot th_diff\n",
    "plt.subplot(1, 3, 2)\n",
    "# plt.plot(dTheta_dz[timeStep], np.arange(0, len(dTheta_dz[timeStep,:])))\n",
    "plt.plot(dTheta_dz[timeStep], z)\n",
    "plt.title('dTheta_dz')\n",
    "plt.xlabel('Value')\n",
    "# plt.ylabel('Depth')\n",
    "\n",
    "# Plot th_flux\n",
    "plt.subplot(1, 3, 3)\n",
    "# plt.plot(wtheta[timeStep], np.arange(0, len(wtheta[timeStep,:])))\n",
    "plt.plot(wtheta[timeStep], z)\n",
    "plt.title('wtheta (th_flux)')\n",
    "# plt.xlabel('Value')\n",
    "# plt.ylabel('Depth')\n",
    "\n",
    "\n",
    "plt.figure(figsize = (12, 6))\n",
    "\n",
    "# Plot potentialTemp\n",
    "plt.subplot(1, 4, 1)\n",
    "# plt.plot(sigma_th[timeStep], np.arange(0, len(sigma_th[timeStep,:])))\n",
    "plt.plot(sigma_th[timeStep], z)\n",
    "plt.title('theta2bar (sigma_th)')\n",
    "# plt.xlabel('Value')\n",
    "plt.ylabel('Depth')\n",
    "\n",
    "# Plot th_diff\n",
    "plt.subplot(1, 4, 2)\n",
    "# plt.plot(sigma_2[timeStep], np.arange(0, len(sigma_2[timeStep,:])))\n",
    "plt.plot(sigma_2[timeStep], z)\n",
    "plt.title('sigma_w^2 (sigma_2)')\n",
    "plt.xlabel('Value')\n",
    "# plt.ylabel('Depth')\n",
    "\n",
    "# Plot th_flux\n",
    "plt.subplot(1, 4, 3)\n",
    "# plt.plot(wwtheta[timeStep], np.arange(0, len(wwtheta[timeStep,:])))\n",
    "plt.plot(wwtheta[timeStep], z)\n",
    "plt.title('(wwtheta)')\n",
    "# plt.xlabel('Value')\n",
    "# plt.ylabel('Depth')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "# plt.plot(dwwtheta_dz[timeStep], np.arange(0, len(dwwtheta_dz[timeStep,:])))\n",
    "plt.plot(dwwtheta_dz[timeStep], z)\n",
    "plt.title('(dwwtheta_dz)')\n",
    "# plt.xlabel('Value')\n",
    "# plt.ylabel('Depth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a555b34-426c-46ef-821f-9e391137ac98",
   "metadata": {},
   "source": [
    "## Diagnosing constants: \n",
    "Often we use conventional constant to normalize profiles. Here are some of the typical ones used in the PBL. \n",
    "\n",
    "AS added note: This modified function calculates the constants once again for a list of different simulations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fddbe7d-ba9d-41ef-b3ca-bca28eb48541",
   "metadata": {},
   "outputs": [],
   "source": [
    "grr = 9.8\n",
    "T_0 = 300\n",
    "beta = grr/T_0\n",
    "\n",
    "def make_constants(path, items, time_avg):\n",
    "    wtheta_surface = []\n",
    "    pbl_height = []\n",
    "    wstar = []\n",
    "    theta_star = []\n",
    "    scaling = []\n",
    "    ustar = []\n",
    "\n",
    "    for item in items:\n",
    "        ds_stat = nc.Dataset(os.path.join(path, item), mode='r')\n",
    "\n",
    "        wtheta_surface_raw = ds_stat.groups['thermo']['th_flux'][:,0] \n",
    "        wtheta_surface_temp = time_average(wtheta_surface_raw, time_avg)\n",
    "\n",
    "        pbl_height_raw = ds_stat.groups['thermo'].variables['zi'][:] \n",
    "        pbl_height_temp = time_average(pbl_height_raw, time_avg)\n",
    "\n",
    "        wstar_raw = np.power( beta * (wtheta_surface_raw) * pbl_height_raw , 1/3) \n",
    "        wstar_temp = time_average(wstar_raw, time_avg)\n",
    "\n",
    "        theta_star_raw = wtheta_surface_raw / wstar_raw\n",
    "        theta_star_temp = time_average(theta_star_raw, time_avg)\n",
    "\n",
    "        scaling_raw = wstar_raw**2 * theta_star_raw / pbl_height_raw\n",
    "        scaling_temp = time_average(scaling_raw, time_avg)\n",
    "\n",
    "        ustar_temp = time_average(ds_stat.groups['default'].variables['ustar'][:], time_avg)\n",
    "\n",
    "        wtheta_surface.append(wtheta_surface_temp)\n",
    "        pbl_height.append(pbl_height_temp)\n",
    "        wstar.append(wstar_temp)\n",
    "        theta_star.append(theta_star_temp)\n",
    "        scaling.append(scaling_temp)\n",
    "        ustar.append(ustar_temp)\n",
    "\n",
    "    wtheta_surface = np.concatenate(wtheta_surface, axis=0)\n",
    "    pbl_height = np.concatenate(pbl_height, axis=0)\n",
    "    wstar = np.concatenate(wstar, axis=0)\n",
    "    theta_star = np.concatenate(theta_star, axis=0)\n",
    "    scaling = np.concatenate(scaling, axis=0)\n",
    "    ustar = np.concatenate(ustar, axis=0)\n",
    "\n",
    "    wtheta_surface = wtheta_surface[:,np.newaxis]\n",
    "    pbl_height = pbl_height[:,np.newaxis]\n",
    "    wstar = wstar[:,np.newaxis]\n",
    "    theta_star = theta_star[:,np.newaxis]\n",
    "    scaling = scaling[:,np.newaxis]\n",
    "    ustar = ustar[:,np.newaxis]\n",
    "\n",
    "    return wtheta_surface, pbl_height, wstar, theta_star, scaling, ustar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3710ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "wtheta_surface, pbl_height, wstar, theta_star, scaling, ustar = make_constants(path, selected_files, time_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffd90e7",
   "metadata": {},
   "source": [
    "Once again printing \n",
    "\n",
    "Name, shape, mean, and standard deviation\n",
    "\n",
    "of the constants (if they are time dependent). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f789ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"wtheta_surface\", wtheta_surface.shape, np.mean(wtheta_surface), np.std(wtheta_surface))\n",
    "print(\"pbl_height\", pbl_height.shape, np.mean(pbl_height), np.std(pbl_height))\n",
    "print(\"wstar\", wstar.shape, np.mean(wstar), np.std(wstar))\n",
    "print(\"theta_star\", theta_star.shape, np.mean(theta_star), np.std(theta_star))\n",
    "print(\"scaling\", scaling.shape, np.mean(scaling), np.std(scaling))\n",
    "print(\"ustar\", ustar.shape, np.mean(ustar), np.std(ustar))\n",
    "print(\"grr\", grr)\n",
    "print(\"T_0\", T_0)\n",
    "print(\"beta\", beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc101d6b-7730-4c39-9297-721f7c7090dd",
   "metadata": {},
   "source": [
    "## Compute the P term as residual\n",
    "Equation 11 in the paper suggest that, assuming that the left hand side is smaller compared to terms on the right hnd side. \n",
    "\n",
    "AS added note: I compute the equation up to 384 as not all variables go up to 385. \n",
    "\n",
    "\n",
    "\n",
    "**Long description:** \n",
    "\n",
    "The equation in question is the heat flux budget in the convective boundary layer, which is written as: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\overline{w \\theta}}{\\partial t}=0=\\underbrace{-\\sigma_w^2 \\frac{\\mathrm{d} \\Theta}{\\mathrm{d} z}}_M \\underbrace{-\\frac{\\mathrm{d} \\overline{w w \\theta}}{\\mathrm{d} z}}_T \\underbrace{-\\frac{1}{\\rho_0} \\theta \\frac{\\mathrm{d} p}{\\mathrm{~d} z}}_P \\underbrace{+\\beta \\overline{\\theta^2}}_B\n",
    "$$\n",
    "\n",
    "Production terms: Mean gradient production ($M$), and bouyancy production ($B$). Destruction terms: turbulent flux transport ($T$) and the the pressure gradient–potential temperature covariance ($P$). \n",
    "\n",
    "$P$ acts as a destruction/sink term for the heat flux $\\overline{w \\theta}$. The general form for the parametrization of $P$ is\n",
    "\n",
    "$$\n",
    "P=-\\frac{1}{\\rho_0} \\overline{\\theta \\frac{\\mathrm{d} p}{\\mathrm{~d} z}}=-C_1 \\frac{\\overline{w \\theta}}{\\tau_1} - C_2 \\beta \\overline{\\theta^2} + C_3 \\sigma_w^2 \\frac{\\mathrm{d} \\Theta}{\\mathrm{d} z}\n",
    "$$\n",
    "\n",
    "The first term $-C_1 \\frac{\\overline{w \\theta}}{\\tau_1}$ is the \"slow\" term. It is Rotta's return to isotropy part. Is inversely proportional to time scale $\\tau_1$. I think it may also be called the \"turbulence-turbulence\" interaction. The last two terms are the \"rapid\" part, with $C_2 \\beta \\overline{\\theta^2}$ being the bouyancy term, and $C_3 \\sigma_w^2 \\frac{\\mathrm{d} \\Theta}{\\mathrm{d} z}$ being the shear-turbulence term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09df8ff7-e9c6-4eb0-a38b-1319081112d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = (- sigma_2 * dTheta_dz)\n",
    "T = - dwwtheta_dz\n",
    "B = (beta * sigma_th)\n",
    "\n",
    "P = - M - T - B\n",
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacf3cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{np.mean(B):.7f}, {np.std(B):.7f}\")\n",
    "print(f\"{np.mean(M):.7f}, {np.std(M):.7f}\")\n",
    "print(f\"{np.mean(T):.7f}, {np.std(T):.7f}\")\n",
    "print(f\"{np.mean(P):.7f}, {np.std(P):.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ae39a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(P[6])\n",
    "min(P[6])\n",
    "\n",
    "min(P[6])/max(P[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f896e0-ada0-4cb1-95f4-efca73d5c968",
   "metadata": {},
   "source": [
    "## Comparing P term and original rotta model plot\n",
    "Lets reproduce plot 6 of the turbulece asymmetry paper to make sure we have all terms looking \"somewhat\" similar. \"somewhat\" because the rotta model may not be very accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83af949e-2e3b-428e-beca-892ca458455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(P[timeStep] * pbl_height[timeStep] / (wstar[timeStep] * theta_star[timeStep]), z)\n",
    "plt.ylim(0,960)\n",
    "plt.title('P')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Depth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ad862d-9b08-4599-8a6f-11b5e1f9a38c",
   "metadata": {},
   "source": [
    "# Cutting out near surface part\n",
    "Some of the assumptions we make when computing P as a residual may not hold near the surface. Therefore, before preparing data for EQ, we remove the near-surface part of the profile, typically the top 7-10 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40ffd7b-203a-4d11-b704-042df4192c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_trim = P[:, 8:155]\n",
    "P_trim.shape\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(P_trim[timeStep], z[8:155])\n",
    "plt.title('P (surface removed)')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Depth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3474a340",
   "metadata": {},
   "source": [
    "## Comparing P to alternative method for calculating it\n",
    "\n",
    "Here we calculate P directly rather than as a residual, and compare the plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ba7f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_subtract = rdstr - transport\n",
    "\n",
    "tau1 = 600 #unsure about this one at the moment, huge number makes it look correct though\n",
    "\n",
    "P_direct = - (3 * wtheta/tau1) - (5/12 * beta * sigma_th) + (2/5 * sigma_2 * dTheta_dz)\n",
    "\n",
    "plt.figure(figsize = (14, 6))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(P_subtract[timeStep] * pbl_height[timeStep] / (wstar[timeStep] * theta_star[timeStep]), z)\n",
    "plt.ylim(45,1000)\n",
    "plt.title('P (rdstr - transport)')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Depth')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(P_direct[timeStep] * pbl_height[timeStep] / (wstar[timeStep] * theta_star[timeStep]), z)\n",
    "plt.title('P (using eq 12, tau = 600)')\n",
    "plt.ylim(45,1000)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Depth')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(P[timeStep] * pbl_height[timeStep] / (wstar[timeStep] * theta_star[timeStep]), z)\n",
    "plt.title('P (residual)')\n",
    "plt.ylim(45,1000)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Depth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2de8f82",
   "metadata": {},
   "source": [
    "Looking at `rdtsr` and `transport` alone is a good sanity check before we abandon their use entirely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028a69dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14, 6))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(rdstr[timeStep] * pbl_height[timeStep] / (wstar[timeStep] * theta_star[timeStep]), z)\n",
    "# plt.ylim(45,1000)\n",
    "plt.title('rdstr alone')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Depth')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(transport[timeStep] * pbl_height[timeStep] / (wstar[timeStep] * theta_star[timeStep]), z)\n",
    "plt.title('transport alone')\n",
    "# plt.ylim(45,1000)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Depth')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(P[timeStep] * pbl_height[timeStep] / (wstar[timeStep] * theta_star[timeStep]), z)\n",
    "plt.title('P (residual)')\n",
    "# plt.ylim(45,1000)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Depth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0dd1b1",
   "metadata": {},
   "source": [
    "Let's put all 3 P's on the same plot: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b3f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot P_subtract\n",
    "plt.plot(P_subtract[timeStep] * pbl_height[timeStep] / (wstar[timeStep] * theta_star[timeStep]), z, label='P (rdstr - transport)')\n",
    "\n",
    "# Plot P_direct\n",
    "plt.plot(P_direct[timeStep] * pbl_height[timeStep] / (wstar[timeStep] * theta_star[timeStep]), z, label='P (direct from eq 12, tau = 700)')\n",
    "\n",
    "# Plot P residual\n",
    "plt.plot(P[timeStep] * pbl_height[timeStep] / (wstar[timeStep] * theta_star[timeStep]), z, label='P (residual)')\n",
    "\n",
    "plt.ylim(45, 1000)\n",
    "plt.title('Comparison of P values')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Depth')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd22b8d",
   "metadata": {},
   "source": [
    "## Equation Discovery\n",
    "\n",
    "We do this with the `PySR` package for symbolic regression. As stated by their github, this is \"a machine learning task where the goal is to find an interpretable symbolic expression that optimizes some objective.\"\n",
    "\n",
    "We import the library down here because sometimes it takes an annoyingly long number of seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acef7d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysr\n",
    "from pysr import PySRRegressor\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f307fcb0",
   "metadata": {},
   "source": [
    "### Step 1: All moments in time, one layer\n",
    "\n",
    "Here we take a single layer, at perhaps the 60th index of z, like `z[60] = 363` across all 120 minutes of time. We will do this for all different versions of $P$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811b50f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "z[60]\n",
    "\n",
    "indices = np.s_[:, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4d847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = P[indices]\n",
    "y1 = P_direct[indices]\n",
    "# y2 = P_subtract[:,60]\n",
    "\n",
    "x0 = wtheta[indices]\n",
    "x1 = sigma_th[indices]\n",
    "x2 = sigma_2[indices]\n",
    "x3 = dTheta_dz[indices]\n",
    "\n",
    "X = np.column_stack([x0, x1, x2, x3])\n",
    "print(y0.shape, y1.shape, X.shape)#, y2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e71aad",
   "metadata": {},
   "source": [
    "Setting up our model with just basic unitary operators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6867bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model0 = PySRRegressor(\n",
    "    niterations=100,  # < Increase me for better results\n",
    "    binary_operators=[\"+\", \"*\", \"-\", \"/\"],\n",
    "    # unary_operators=[\n",
    "    #     \"cos\",\n",
    "    #     \"exp\",\n",
    "    #     \"sin\",\n",
    "    #     \"inv(x) = 1/x\",\n",
    "    #     \"square\",\n",
    "    #     \"cube\",\n",
    "    #     # ^ Custom operator (julia syntax)\n",
    "    # ],\n",
    "    extra_sympy_mappings={\"inv\": lambda x: 1 / x},\n",
    "    # ^ Define operator for SymPy as well\n",
    "    elementwise_loss=\"loss(prediction, target) = (prediction - target)^2\",\n",
    "    # ^ Custom loss function (julia syntax)\n",
    "    # batching = True,\n",
    "    # batch_size = 1000,\n",
    ")\n",
    "\n",
    "model1 = PySRRegressor(\n",
    "    niterations=100,  # < Increase me for better results\n",
    "    binary_operators=[\"+\", \"*\", \"-\", \"/\"],\n",
    "    # unary_operators=[\n",
    "    #     \"cos\",\n",
    "    #     \"exp\",\n",
    "    #     \"sin\",\n",
    "    #     \"inv(x) = 1/x\",\n",
    "    #     \"square\",\n",
    "    #     \"cube\",\n",
    "    #     # ^ Custom operator (julia syntax)\n",
    "    # ],\n",
    "    extra_sympy_mappings={\"inv\": lambda x: 1 / x},\n",
    "    # ^ Define operator for SymPy as well\n",
    "    elementwise_loss=\"loss(prediction, target) = (prediction - target)^2\",\n",
    "    # ^ Custom loss function (julia syntax)\n",
    "    # batching = True,\n",
    "    # batch_size = 1000,\n",
    ")\n",
    "\n",
    "# model2 = PySRRegressor(\n",
    "#     niterations=100,  # < Increase me for better results\n",
    "#     binary_operators=[\"+\", \"*\", \"-\", \"/\"],\n",
    "#     # unary_operators=[\n",
    "#     #     \"cos\",\n",
    "#     #     \"exp\",\n",
    "#     #     \"sin\",\n",
    "#     #     \"inv(x) = 1/x\",\n",
    "#     #     \"square\",\n",
    "#     #     \"cube\",\n",
    "#     #     # ^ Custom operator (julia syntax)\n",
    "#     # ],\n",
    "#     extra_sympy_mappings={\"inv\": lambda x: 1 / x},\n",
    "#     # ^ Define operator for SymPy as well\n",
    "#     elementwise_loss=\"loss(prediction, target) = (prediction - target)^2\",\n",
    "#     # ^ Custom loss function (julia syntax)\n",
    "#     # batching = True,\n",
    "#     # batch_size = 1000,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb34c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model0.fit(X,y0)\n",
    "model1.fit(X,y1)\n",
    "# model2.fit(X,y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2016beaa",
   "metadata": {},
   "source": [
    "Looking at last equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080dfbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in (model0, model1): #, model2):\n",
    "    equations = model.equations_\n",
    "    print(equations.shape)\n",
    "    display(equations)\n",
    "    print(model, \"Final Equation: \")\n",
    "    display(equations.iloc[-1, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb743a6",
   "metadata": {},
   "source": [
    "### Step 2: One moment in time, a few layers\n",
    "\n",
    "This is probably too simple, but we'll see. Just going to take z from 60 to 80, at one point in time, and see what we get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd026f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "wtheta[14,60:80]\n",
    "\n",
    "indices = np.s_[14, 60:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0a5d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = P[indices]\n",
    "y1 = P_direct[indices]\n",
    "# y2 = P_subtract[:,60]\n",
    "\n",
    "x0 = wtheta[indices]\n",
    "x1 = sigma_th[indices]\n",
    "x2 = sigma_2[indices]\n",
    "x3 = dTheta_dz[indices]\n",
    "\n",
    "X = np.column_stack([x0, x1, x2, x3])\n",
    "print(y0.shape, y1.shape, X.shape)#, y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86694566",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = PySRRegressor(\n",
    "    niterations=100,  # < Increase me for better results\n",
    "    binary_operators=[\"+\", \"*\", \"-\", \"/\"],\n",
    "    # unary_operators=[\n",
    "    #     \"cos\",\n",
    "    #     \"exp\",\n",
    "    #     \"sin\",\n",
    "    #     \"inv(x) = 1/x\",\n",
    "    #     \"square\",\n",
    "    #     \"cube\",\n",
    "    #     # ^ Custom operator (julia syntax)\n",
    "    # ],\n",
    "    extra_sympy_mappings={\"inv\": lambda x: 1 / x},\n",
    "    # ^ Define operator for SymPy as well\n",
    "    elementwise_loss=\"loss(prediction, target) = (prediction - target)^2\",\n",
    "    # ^ Custom loss function (julia syntax)\n",
    "    # batching = True,\n",
    "    # batch_size = 1000,\n",
    ")\n",
    "\n",
    "model4 = PySRRegressor(\n",
    "    niterations=100,  # < Increase me for better results\n",
    "    binary_operators=[\"+\", \"*\", \"-\", \"/\"],\n",
    "    # unary_operators=[\n",
    "    #     \"cos\",\n",
    "    #     \"exp\",\n",
    "    #     \"sin\",\n",
    "    #     \"inv(x) = 1/x\",\n",
    "    #     \"square\",\n",
    "    #     \"cube\",\n",
    "    #     # ^ Custom operator (julia syntax)\n",
    "    # ],\n",
    "    extra_sympy_mappings={\"inv\": lambda x: 1 / x},\n",
    "    # ^ Define operator for SymPy as well\n",
    "    elementwise_loss=\"loss(prediction, target) = (prediction - target)^2\",\n",
    "    # ^ Custom loss function (julia syntax)\n",
    "    # batching = True,\n",
    "    # batch_size = 1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2351b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.fit(X,y0)\n",
    "model4.fit(X,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0995ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in (model3, model4):\n",
    "    equations = model.equations_\n",
    "    print(equations.shape)\n",
    "    display(equations)\n",
    "    print(model, \"Final Equation: \")\n",
    "    display(equations.iloc[-1, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ab4670",
   "metadata": {},
   "source": [
    "### Step 3: All moments in time, a few layers\n",
    "\n",
    "This is my current best bet. Might have to start using batching here. I think in the others we have too little data. Will take heights 60 to 80 and all instances in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389ffbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dTheta_dz[:, 60:80].shape, dTheta_dz[:, 60:80].ravel().shape)\n",
    "\n",
    "indices = np.s_[:, 60:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eabd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = P[indices].ravel()\n",
    "y1 = P_direct[indices].ravel()\n",
    "# y2 = P_subtract[:,60]\n",
    "\n",
    "x0 = wtheta[indices].ravel()\n",
    "x1 = sigma_th[indices].ravel()\n",
    "x2 = sigma_2[indices].ravel()\n",
    "x3 = dTheta_dz[indices].ravel()\n",
    "\n",
    "\n",
    "\n",
    "X = np.column_stack([x0, x1, x2, x3])\n",
    "print(y0.shape, y1.shape, X.shape)#, y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9370e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = PySRRegressor(\n",
    "    niterations=100,  # < Increase me for better results\n",
    "    binary_operators=[\"+\", \"*\", \"-\", \"/\"],\n",
    "    # unary_operators=[\n",
    "    #     \"cos\",\n",
    "    #     \"exp\",\n",
    "    #     \"sin\",\n",
    "    #     \"inv(x) = 1/x\",\n",
    "    #     \"square\",\n",
    "    #     \"cube\",\n",
    "    #     # ^ Custom operator (julia syntax)\n",
    "    # ],\n",
    "    extra_sympy_mappings={\"inv\": lambda x: 1 / x},\n",
    "    # ^ Define operator for SymPy as well\n",
    "    elementwise_loss=\"loss(prediction, target) = (prediction - target)^2\",\n",
    "    # ^ Custom loss function (julia syntax)\n",
    "    # batching = True,\n",
    "    # batch_size = 1000,\n",
    ")\n",
    "\n",
    "model6 = PySRRegressor(\n",
    "    niterations=100,  # < Increase me for better results\n",
    "    binary_operators=[\"+\", \"*\", \"-\", \"/\"],\n",
    "    # unary_operators=[\n",
    "    #     \"cos\",\n",
    "    #     \"exp\",\n",
    "    #     \"sin\",\n",
    "    #     \"inv(x) = 1/x\",\n",
    "    #     \"square\",\n",
    "    #     \"cube\",\n",
    "    #     # ^ Custom operator (julia syntax)\n",
    "    # ],\n",
    "    extra_sympy_mappings={\"inv\": lambda x: 1 / x},\n",
    "    # ^ Define operator for SymPy as well\n",
    "    elementwise_loss=\"loss(prediction, target) = (prediction - target)^2\",\n",
    "    # ^ Custom loss function (julia syntax)\n",
    "    # batching = True,\n",
    "    # batch_size = 1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab3591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.fit(X,y0)\n",
    "model6.fit(X,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75da76c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in (model5, model6):\n",
    "    equations = model.equations_\n",
    "    print(equations.shape)\n",
    "    display(equations)\n",
    "    print(model, \"Final Equation: \")\n",
    "    display(equations.iloc[-1, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65211cab",
   "metadata": {},
   "source": [
    "timescale is in seconds so 600 or more is reasonable, on the order of 10 min or so. also $\\frac{\\text{h}}{\\text{wstar}} = \\tau$. $\\tau$ is the timescale of circulation. h is `pbl_height`, and is the \"length scale\", while wstar is `wstar` which is the \"velocity scale\"\n",
    "\n",
    "play with different symbolic regression parameters/settings (complexity, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a54a36",
   "metadata": {},
   "source": [
    "### Step 4: All moments in time, most layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e6ebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.s_[:, 0:200]\n",
    "\n",
    "y = P[indices].ravel()\n",
    "\n",
    "x0 = wtheta[indices].ravel()\n",
    "x1 = sigma_th[indices].ravel()\n",
    "x2 = sigma_2[indices].ravel()\n",
    "x3 = dTheta_dz[indices].ravel()\n",
    "\n",
    "\n",
    "\n",
    "X = np.column_stack([x0, x1, x2, x3])\n",
    "print(y.shape, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8683e668",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = pd.DataFrame(X, columns=['wtheta', 'sigma_th', 'sigma_2', 'dTheta_dz'])\n",
    "df_y = pd.DataFrame(y, columns=['P'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43794ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model7 = PySRRegressor(\n",
    "    niterations=2000,  # < Increase me for better results\n",
    "    binary_operators=[\"+\", \"*\", \"-\", \"/\"],\n",
    "    unary_operators=[\n",
    "        \"cos\",\n",
    "        \"exp\",\n",
    "        \"sin\",\n",
    "        \"inv(x) = 1/x\",\n",
    "        \"square\",\n",
    "        \"cube\",\n",
    "        # ^ Custom operator (julia syntax)\n",
    "    ],\n",
    "    extra_sympy_mappings={\"inv\": lambda x: 1 / x},\n",
    "    # ^ Define operator for SymPy as well\n",
    "    elementwise_loss=\"loss(prediction, target) = (prediction - target)^2\",\n",
    "    # ^ Custom loss function (julia syntax)\n",
    "    batching = True,\n",
    "    batch_size = 200,\n",
    "    complexity_of_operators={\"/\": 1, \"exp\": 3, \"*\":1, \"sin\": 3, \"cos\": 3, \"inv\": 3, \"square\": 3, \"cube\": 3},\n",
    "    # ^ Custom complexity of particular operators\n",
    "    maxdepth=5,\n",
    "    # ^ But, avoid deep nesting.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16496c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model7.fit(df_X,df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8103c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [model7]:\n",
    "    equations = model.equations_\n",
    "    print(equations.shape)\n",
    "    display(equations)\n",
    "    print(\"Final Equation: \")\n",
    "    display(equations.iloc[-1, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc8cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "discovered_eq1 = (-0.004997506 * wtheta) - (sigma_th * 0.019068547)\n",
    "discovered_eq2 = (((sigma_th * wtheta) + (sigma_2 * 0.4073766)) * dTheta_dz) - (0.02337299 * (sigma_th + (0.13268535 * wtheta)))\n",
    "\n",
    "theoretical_eq = (0.4073766 * sigma_2 * dTheta_dz) - (0.02337299 *sigma_th) + (0.0031012533586965 * wtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fbbfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discovered value in front of sigma_th is -0.019068547\n",
    "C2 = 1/2\n",
    "\n",
    "- C2*beta\n",
    "#close enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4116b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#discovered value in front of wtheta is -0.004997506\n",
    "\n",
    "C1 = 3\n",
    "print(- C1/tau1)\n",
    "\n",
    "\n",
    "\n",
    "print(C1/0.004997506)\n",
    "\n",
    "#reasonable tau is about 600, which is exactly what we have\n",
    "\n",
    "pbl_height/wstar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736ca77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{np.mean(wtheta) * -0.019068547:.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725a43c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{np.mean(sigma_th) * -0.004997506:.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf26888",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{np.mean(sigma_2 * dTheta_dz) * 2/5:.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac94189",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeStep = 10\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(P[timeStep], z, label = 'P (residual, eq 11)')\n",
    "# plt.plot(wtheta[timeStep] * -0.019068547, z, label = 'wtheta')\n",
    "# plt.plot(sigma_th[timeStep] * -0.004997506, z, label = 'sigma_th')\n",
    "# plt.plot(sigma_2[timeStep] * dTheta_dz[timeStep] * 2/5, z, label = 'sigma_2 * dTheta_dz')\n",
    "\n",
    "#if u make the third term negative the game changes\n",
    "plt.plot((- sigma_th[timeStep] * 0.004997506) + (- wtheta[timeStep] * 0.019068547) - (sigma_2[timeStep] * dTheta_dz[timeStep] * 2/5), z, label = '3 term sum', color = 'darkred')\n",
    "plt.plot(discovered_eq2[timeStep], z, label = 'discovered eq2', color = 'red')\n",
    "plt.plot(discovered_eq1[timeStep], z, label = 'discovered eq1', color = 'green')\n",
    "\n",
    "plt.plot(theoretical_eq[timeStep], z, label = 'theoretical eq', color = 'goldenrod')\n",
    "\n",
    "plt.plot(P_direct[timeStep], z, label = 'P (direct, eq12)')\n",
    "\n",
    "plt.ylim(0, 1000)\n",
    "plt.title('Comparison of P values')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Depth')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e144569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y, model7.predict(X))\n",
    "plt.xlabel('Truth')\n",
    "plt.ylabel('Prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a88e0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wtheta_hmin(path, items, time_avg):\n",
    "    \"\"\"\n",
    "    Extracts the variables of interest from the LES simulations and averages them over specified number of time steps.\n",
    "\n",
    "    Parameters:\n",
    "    - path (str): The path to the LES simulation data.\n",
    "    - items (list): A list of files containing the LES simulation data.\n",
    "    - time_avg (int): The number of time steps over which to average.\n",
    "\n",
    "    Returns:\n",
    "    - time averaged wthetas\n",
    "    - time averaged hmins\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize empty lists to accumulate results\n",
    "    hmin = []\n",
    "    wtheta_hmin = []\n",
    "\n",
    "    for item in items:\n",
    "\n",
    "        hmin_temp = []\n",
    "        wtheta_hmin_temp = []\n",
    "\n",
    "        ds_stat = nc.Dataset(os.path.join(path, item), mode='r')\n",
    "        wtheta = ds_stat.groups['thermo']['th_flux'][:]\n",
    "        zh = ds_stat.groups['thermo']['zh'][:]\n",
    "\n",
    "        for t in range(wtheta.shape[0]):\n",
    "            wtheta_at_t = wtheta[t]\n",
    "            min_wtheta = np.min(wtheta_at_t)\n",
    "            min_height = zh[np.argmin(wtheta_at_t)]\n",
    "            hmin_temp.append(min_height)\n",
    "            wtheta_hmin_temp.append(min_wtheta)\n",
    "\n",
    "        hmin__temp_timeavg = time_average(hmin_temp, time_avg)\n",
    "        wtheta_hmin_temp_timeavg = time_average(wtheta_hmin_temp, time_avg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Append the results to the respective lists\n",
    "        hmin.append(hmin__temp_timeavg)\n",
    "        wtheta_hmin.append(wtheta_hmin_temp_timeavg)\n",
    "\n",
    "    # Concatenate the results along the time axis\n",
    "    hmin = np.concatenate(hmin, axis=0)\n",
    "    wtheta_hmin = np.concatenate(wtheta_hmin, axis=0)\n",
    "\n",
    "    return hmin, wtheta_hmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "167a70d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[876.0, 882.0, 882.0, 870.0, 858.0, 870.0, 846.0, 900.0, 876.0, 900.0, 882.0, 870.0, 858.0, 912.0, 918.0, 894.0, 876.0, 864.0, 876.0, 894.0, 930.0, 972.0, 960.0, 936.0, 930.0, 906.0, 918.0, 924.0, 900.0, 918.0, 918.0, 936.0, 924.0, 906.0, 906.0, 918.0, 942.0, 948.0, 960.0, 954.0, 936.0, 984.0, 984.0, 996.0, 990.0, 894.0, 936.0, 918.0, 906.0, 888.0, 876.0, 846.0, 984.0, 936.0, 936.0, 954.0, 930.0, 900.0, 912.0, 918.0, 912.0, 912.0, 966.0, 966.0, 978.0, 954.0, 966.0, 936.0, 954.0, 942.0, 966.0, 936.0, 924.0, 906.0, 972.0, 966.0, 948.0, 924.0, 924.0, 948.0, 972.0, 1014.0, 1008.0, 1002.0, 984.0, 960.0, 978.0, 972.0, 972.0, 954.0, 978.0, 1002.0, 1044.0, 966.0, 954.0, 990.0, 966.0, 984.0, 984.0, 972.0, 954.0, 948.0, 966.0, 990.0, 1050.0, 1026.0, 954.0, 1002.0, 990.0, 948.0, 954.0, 924.0, 930.0, 936.0, 1026.0, 1014.0, 996.0, 978.0, 1038.0, 1020.0]\n",
      "[-0.01394034040636906, -0.01339567088252956, -0.01220588938803464, -0.011673264932567241, -0.011677776304173326, -0.012340427794862817, -0.01211350621672476, -0.013671514597229413, -0.013608573396058907, -0.013516379160990747, -0.013942205903026306, -0.01431182673685029, -0.014269876567460281, -0.015805390228185133, -0.017516947837766694, -0.01733568493663693, -0.01631553567334517, -0.015981253713055206, -0.01540361837446345, -0.016803811514143913, -0.017986627755611276, -0.01783129147795271, -0.017276939986567947, -0.015534477811436806, -0.015876031455837814, -0.01649441039925797, -0.01780561443961728, -0.01904295856042949, -0.018306081423170258, -0.016852741675911498, -0.016569371340387, -0.01608599361928491, -0.016731220244796646, -0.01554777136608235, -0.014416813785137799, -0.014319447905347095, -0.015173485141798084, -0.016168795067376545, -0.018760722141201117, -0.016894265107696364, -0.015470994123205697, -0.01485811614807963, -0.016132935047902038, -0.01439981867763957, -0.012993880673031224, -0.013027906234050946, -0.012714148406650994, -0.012373968380101118, -0.011786005061495401, -0.010972433569691268, -0.009734271036844738, -0.009226031946466526, -0.00934970954725657, -0.010965079055537504, -0.011373477245530771, -0.010977204433916886, -0.011029674887765278, -0.010122101854739525, -0.010058153742307497, -0.011101892222801623, -0.01125019555403508, -0.010620339720973168, -0.010555834542703216, -0.011120327808479807, -0.01107228261422485, -0.01120255613901224, -0.011562829089621186, -0.01247906805490468, -0.012334065355229256, -0.012791386727322863, -0.01344270279830254, -0.013420812056444636, -0.013137141388582291, -0.012447876671784926, -0.01304981327476147, -0.012671300189514382, -0.012447921891684623, -0.012693200593301682, -0.011948376649658312, -0.011449890037445022, -0.01186551404331767, -0.012235258960859608, -0.01421989133207617, -0.015059927355398111, -0.013816592300528218, -0.01213781492403994, -0.012281651367337571, -0.011950570216228097, -0.010819153513114703, -0.010106551882648427, -0.01028843425692193, -0.010783450855636688, -0.010174132039646025, -0.009502137067363605, -0.01024117367441435, -0.010829146414198292, -0.010541383766228324, -0.010470993265995352, -0.010964403442863563, -0.010034215122698512, -0.00922033573158949, -0.008874412889933455, -0.008153571136448816, -0.008097496359053432, -0.00894856053227807, -0.009377036105474822, -0.009838526364156068, -0.010640605146469417, -0.010462901477008027, -0.008653591426150863, -0.008309944149897455, -0.00866200669219593, -0.00936810451338167, -0.009505662045810177, -0.009441172354165469, -0.010136952827441238, -0.010891636928188397, -0.010595044513100796, -0.010249684284832968, -0.011003107573395501]\n"
     ]
    }
   ],
   "source": [
    "path = 'C:/Users/anton/Desktop/Career/LEAP_nyc_Summer2024/les_sim_2/'\n",
    "\n",
    "# example_df = nc.Dataset(os.path.join(path, 'Ug16Q003_IV.nc'), mode='r')\n",
    "\n",
    "# wtheta_temp = example_df.groups['thermo']['th_flux'][:]\n",
    "# zh = example_df.variables['zh'][:]\n",
    "# wtheta_temp.shape\n",
    "\n",
    "# hmin_temp = []\n",
    "# wtheta_min_temp = []\n",
    "\n",
    "# for t in range(wtheta_temp.shape[0]):\n",
    "#     wtheta_at_t = wtheta_temp[t]\n",
    "#     min_wtheta = np.min(wtheta_at_t)\n",
    "#     min_height = zh[np.argmin(wtheta_at_t)]\n",
    "#     hmin_temp.append(min_height)\n",
    "#     wtheta_min_temp.append(min_wtheta)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423761fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wtheta_hmin(path, items, time_avg):\n",
    "    \"\"\"\n",
    "    Extracts the variables of interest from the LES simulations and averages them over specified number of time steps.\n",
    "\n",
    "    Parameters:\n",
    "    - path (str): The path to the LES simulation data.\n",
    "    - items (list): A list of file names containing the LES simulation data. Ex: ['Ug16Q001_IV.nc', 'Ug16Q003_IV.nc', 'Ug16Q006_I.nc']\n",
    "    - time_avg (int): The number of time steps over which to average.\n",
    "\n",
    "    Returns:\n",
    "    - time averaged wtheta_hmins\n",
    "    - time averaged hmins\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize empty lists to accumulate results\n",
    "    hmin = []\n",
    "    wtheta_hmin = []\n",
    "\n",
    "    for item in items:\n",
    "\n",
    "        hmin_temp = []\n",
    "        wtheta_hmin_temp = []\n",
    "\n",
    "        ds_stat = nc.Dataset(os.path.join(path, item), mode='r')\n",
    "        wtheta = ds_stat.groups['thermo']['th_flux'][:]\n",
    "        zh = ds_stat.variables['zh'][:]\n",
    "\n",
    "        for t in range(wtheta.shape[0]):\n",
    "            wtheta_at_t = wtheta[t]\n",
    "            min_wtheta = np.min(wtheta_at_t)\n",
    "            min_height = zh[np.argmin(wtheta_at_t)]\n",
    "            hmin_temp.append(min_height)\n",
    "            wtheta_hmin_temp.append(min_wtheta)\n",
    "\n",
    "        hmin__temp_timeavg = time_average(np.array(hmin_temp), time_avg)\n",
    "        wtheta_hmin_temp_timeavg = time_average(np.array(wtheta_hmin_temp), time_avg)\n",
    "\n",
    "        # Append the results to the respective lists\n",
    "        hmin.append(hmin__temp_timeavg)\n",
    "        wtheta_hmin.append(wtheta_hmin_temp_timeavg)\n",
    "\n",
    "    # Concatenate the results along the time axis\n",
    "    hmin = np.concatenate(hmin, axis=0)\n",
    "    wtheta_hmin = np.concatenate(wtheta_hmin, axis=0)\n",
    "\n",
    "    return hmin, wtheta_hmin\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#actually using the function here \n",
    "hmin, wtheta_min = make_wtheta_hmin(path, selected_files, time_avg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

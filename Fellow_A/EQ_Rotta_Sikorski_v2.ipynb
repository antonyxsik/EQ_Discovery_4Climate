{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotta EQ Discovery\n",
    "\n",
    "*Authors: Antony Sikorski*\n",
    "\n",
    "A simpler notebook for EQ discovery for the Rotta model. We do this with the `PySR` package for symbolic regression, an ML method for finding interpretable symbolic expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "# from pdfs import *\n",
    "import os\n",
    "import re\n",
    "\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "import h5netcdf\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "import pysr\n",
    "from pysr import PySRRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_directories_files(path):\n",
    "    \"\"\"\n",
    "    provides all files and directories from the LES simulations (they start with Ug)\n",
    "\n",
    "    Parameters:\n",
    "    - path (str): The path to the LES simulation data.\n",
    "\n",
    "    Returns:\n",
    "    - directories (list): A list of all directories starting with 'Ug'.\n",
    "    - files (list): A list of all files starting with 'Ug'.\n",
    "    \"\"\"\n",
    "\n",
    "    # all items in the given path\n",
    "    items = os.listdir(path)\n",
    "    \n",
    "    # find the directories starting with 'Ug'\n",
    "    directories = [item for item in items if os.path.isdir(os.path.join(path, item)) and item.startswith('Ug')]\n",
    "    # find the files starting with 'Ug'\n",
    "    files = [item for item in items if os.path.isfile(os.path.join(path, item)) and item.startswith('Ug')]\n",
    "    \n",
    "    return directories, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_average(data, timeavg):\n",
    "    \"\"\"\n",
    "    Averages an array over specified number of time steps. Works for both 1D and higher-dimensional arrays.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): The input data array. Expected shapes are either (ntime,) or (ntime, nz).\n",
    "    - timeavg (int): The number of time steps over which to average.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The averaged data array.\n",
    "    \"\"\"\n",
    "    ntime = data.shape[0]\n",
    "    nchunks = ntime // timeavg\n",
    "    truncated_data = data[:nchunks * timeavg]\n",
    "\n",
    "    if data.ndim == 1:\n",
    "        # for 1D arrays, reshape to (nchunks, timeavg)\n",
    "        reshaped_data = truncated_data.reshape(nchunks, timeavg)\n",
    "    else:\n",
    "        # for 2D arrays, old method with reshaping\n",
    "        nz = data.shape[1]\n",
    "        reshaped_data = truncated_data.reshape(nchunks, timeavg, nz)\n",
    "\n",
    "    # compute the mean along the new time axis \n",
    "    averaged_data = reshaped_data.mean(axis=1)\n",
    "\n",
    "    return averaged_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_variables(path, items, time_avg):\n",
    "    \"\"\"\n",
    "    Extracts the variables of interest from the LES simulations and averages them over specified number of time steps.\n",
    "\n",
    "    Parameters:\n",
    "    - path (str): The path to the LES simulation data.\n",
    "    - items (list): A list of files containing the LES simulation data.\n",
    "    - time_avg (int): The number of time steps over which to average.\n",
    "\n",
    "    Returns:\n",
    "    - all variables (numpy.ndarray)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize empty lists to accumulate results\n",
    "    sigma_th = []\n",
    "    sigma_2 = []\n",
    "    Theta = []\n",
    "    wtheta = []\n",
    "    wwtheta = []\n",
    "    rdstr = []\n",
    "    transport = []\n",
    "\n",
    "    for item in items:\n",
    "        ds_stat = nc.Dataset(os.path.join(path, item), mode='r')\n",
    "        \n",
    "        sigma_th_temp = time_average(ds_stat.groups['thermo']['th_2'][:], time_avg)  # covariance of theta\n",
    "        sigma_2_temp = time_average(ds_stat.groups['default']['w_2'][:], time_avg)   # covariance of w\n",
    "        Theta_temp = time_average(ds_stat.groups['thermo']['th'][:], time_avg)       # domain mean theta\n",
    "        wtheta_temp = time_average(ds_stat.groups['thermo']['th_flux'][:], time_avg) # heat flux\n",
    "        wwtheta_temp = time_average(ds_stat.groups['budget']['wwtheta'][:], time_avg) # third moment, covariance between wtheta and w\n",
    "        rdstr_temp = time_average(ds_stat.groups['budget']['bw_rdstr'][:], time_avg) # pressure redistribution term\n",
    "        transport_temp = time_average(ds_stat.groups['budget']['bw_pres'][:], time_avg) # pressure transport term\n",
    "\n",
    "        # Append the results to the respective lists\n",
    "        sigma_th.append(sigma_th_temp)\n",
    "        sigma_2.append(sigma_2_temp)\n",
    "        Theta.append(Theta_temp)\n",
    "        wtheta.append(wtheta_temp)\n",
    "        wwtheta.append(wwtheta_temp)\n",
    "        rdstr.append(rdstr_temp)\n",
    "        transport.append(transport_temp)\n",
    "\n",
    "    # Concatenate the results along the time axis\n",
    "    sigma_th = np.concatenate(sigma_th, axis=0)\n",
    "    sigma_2 = np.concatenate(sigma_2, axis=0)\n",
    "    Theta = np.concatenate(Theta, axis=0)\n",
    "    wtheta = np.concatenate(wtheta, axis=0)\n",
    "    wwtheta = np.concatenate(wwtheta, axis=0)\n",
    "    rdstr = np.concatenate(rdstr, axis=0)\n",
    "    transport = np.concatenate(transport, axis=0)\n",
    "\n",
    "    return sigma_th, sigma_2, Theta, wtheta, wwtheta, rdstr, transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_variables(variable):\n",
    "    reshaped = (variable[:, :-1] + variable[:, 1:]) / 2.0\n",
    "    return reshaped\n",
    "\n",
    "\n",
    "def extract_ug_q(filename):\n",
    "    # Extracting 'ug' and 'q' from the filename\n",
    "    match = re.search(r'Ug(\\d+)Q(\\d+)', filename)\n",
    "    ug = int(match.group(1))\n",
    "    q = int(match.group(2))\n",
    "\n",
    "    return ug, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_constants(path, items, time_avg):\n",
    "    \"\"\"\n",
    "    Extracts the constants of interest from the LES simulations and averages them over specified number of time steps.\n",
    "\n",
    "    Parameters:\n",
    "    - path (str): The path to the LES simulation data.\n",
    "    - items (list): A list of files containing the LES simulation data.\n",
    "    - time_avg (int): The number of time steps over which to average.\n",
    "\n",
    "    Returns:\n",
    "    - all constants (numpy.ndarray)\n",
    "    \"\"\"\n",
    "    wtheta_surface = []\n",
    "    pbl_height = []\n",
    "    wstar = []\n",
    "    theta_star = []\n",
    "    scaling = []\n",
    "    ustar = []\n",
    "    grr = 9.8\n",
    "    T_0 = 300\n",
    "    beta = grr/T_0\n",
    "    ug = []\n",
    "    q = []\n",
    "\n",
    "    for item in items:\n",
    "        ds_stat = nc.Dataset(os.path.join(path, item), mode='r')\n",
    "\n",
    "        wtheta_surface_raw = ds_stat.groups['thermo']['th_flux'][:,0] \n",
    "        wtheta_surface_temp = time_average(wtheta_surface_raw, time_avg)\n",
    "\n",
    "        pbl_height_raw = ds_stat.groups['thermo'].variables['zi'][:] \n",
    "        pbl_height_temp = time_average(pbl_height_raw, time_avg)\n",
    "\n",
    "        wstar_raw = np.power( beta * (wtheta_surface_raw) * pbl_height_raw , 1/3) \n",
    "        wstar_temp = time_average(wstar_raw, time_avg)\n",
    "\n",
    "        theta_star_raw = wtheta_surface_raw / wstar_raw\n",
    "        theta_star_temp = time_average(theta_star_raw, time_avg)\n",
    "\n",
    "        scaling_raw = wstar_raw**2 * theta_star_raw / pbl_height_raw\n",
    "        scaling_temp = time_average(scaling_raw, time_avg)\n",
    "\n",
    "        ustar_temp = time_average(ds_stat.groups['default'].variables['ustar'][:], time_avg)\n",
    "\n",
    "        ug_val, q_val = extract_ug_q(item)\n",
    "        # ug = np.ma.masked_array(np.full(wtheta_surface_temp.shape, ug_val), mask=wtheta_surface_temp.mask)\n",
    "        # q = np.ma.masked_array(np.full(wtheta_surface_temp.shape, q_val), mask=wtheta_surface_temp.mask)\n",
    "        ug_repeat = np.full(wtheta_surface_temp.shape, ug_val)\n",
    "        q_repeat = np.full(wtheta_surface_temp.shape, q_val)\n",
    "\n",
    "        wtheta_surface.append(wtheta_surface_temp)\n",
    "        pbl_height.append(pbl_height_temp)\n",
    "        wstar.append(wstar_temp)\n",
    "        theta_star.append(theta_star_temp)\n",
    "        scaling.append(scaling_temp)\n",
    "        ustar.append(ustar_temp)\n",
    "        ug.append(ug_repeat)\n",
    "        q.append(q_repeat)\n",
    "\n",
    "    wtheta_surface = np.concatenate(wtheta_surface, axis=0)\n",
    "    pbl_height = np.concatenate(pbl_height, axis=0)\n",
    "    wstar = np.concatenate(wstar, axis=0)\n",
    "    theta_star = np.concatenate(theta_star, axis=0)\n",
    "    scaling = np.concatenate(scaling, axis=0)\n",
    "    ustar = np.concatenate(ustar, axis=0)\n",
    "    ug = np.concatenate(ug, axis=0)\n",
    "    q = np.concatenate(q, axis=0)\n",
    "\n",
    "    wtheta_surface = wtheta_surface[:,np.newaxis]\n",
    "    pbl_height = pbl_height[:,np.newaxis]\n",
    "    wstar = wstar[:,np.newaxis]\n",
    "    theta_star = theta_star[:,np.newaxis]\n",
    "    scaling = scaling[:,np.newaxis]\n",
    "    ustar = ustar[:,np.newaxis]\n",
    "    ug = ug[:,np.newaxis]\n",
    "    q = q[:,np.newaxis]\n",
    "\n",
    "    return wtheta_surface, pbl_height, wstar, theta_star, scaling, ustar, grr, T_0, beta, ug, q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lists all of our files. **If someone else is using this notebook, all that needs to be changed to reproduce results is the path.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/anton/Desktop/Career/LEAP_nyc_Summer2024/les_sim_2/'\n",
    "\n",
    "\n",
    "directories, files = list_directories_files(path)\n",
    "print(\"Directories starting with 'Ug':\", directories)\n",
    "print(\"Files starting with 'Ug':\", files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which ones have the budget category (we need variables from it): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in items:\n",
    "    ds_stat = nc.Dataset( os.path.join(path, item), mode='r')\n",
    "    if 'budget' in ds_stat.groups:\n",
    "        print (\"budget is in\", item)\n",
    "    else:\n",
    "        print (\"budget is not in\", item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that we can use all files except the very first (`000`) file. Going to print out all of the viable files so we can see dimension sizes and groups: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in items[1:]:\n",
    "    print(item)\n",
    "    df = nc.Dataset(path + '/' + item, mode='r')\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should create selected files for a given list of `items`. In my case, my `selected_files` constitutes a group of similar (I think) simulations that I can append together for the sake of having more data. For now (for ease of use) I want them all to have the same `z` and `zh` dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_z_dim = 384\n",
    "\n",
    "selected_files = []\n",
    "\n",
    "for item in items[1:]:\n",
    "        df = nc.Dataset(os.path.join(path, item), mode='r')\n",
    "        if df.dimensions['z'].size == target_z_dim:\n",
    "            selected_files.append(item)\n",
    "        df.close()\n",
    "\n",
    "print(len(selected_files))\n",
    "print(selected_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at all of the variables inside an example file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df = nc.Dataset(os.path.join(path, selected_files[1]), mode='r')\n",
    "\n",
    "for group_name, group in example_df.groups.items():\n",
    "    print(f\"Group: {group_name}\")\n",
    "    for var_name, var in group.variables.items():\n",
    "        long_name = var.getncattr('long_name') if 'long_name' in var.ncattrs() else 'No long_name attribute'\n",
    "        print(f\"  Variable: {var_name}, Long Name: {long_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wtheta_surface_raw = example_df.groups['thermo']['th_flux'][:,0] \n",
    "\n",
    "# test = time_average(wtheta_surface_raw, 15)\n",
    "\n",
    "# print(test.shape)\n",
    "\n",
    "# ug = 4\n",
    "\n",
    "# time_average(wtheta_surface_raw,15)\n",
    "\n",
    "# ug = np.ma.masked_array(np.full(test.shape, ug), mask=test.mask)\n",
    "# # q = np.ma.masked_array(np.full(wstar_temp.shape, q_val), mask=wstar_temp.mask)\n",
    "\n",
    "# ug\n",
    "\n",
    "# ug.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Variables and Constants\n",
    "\n",
    "We start off with the necessary variables. First we must decide on a time averaging parameter. We will use 15 minutes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choosing a time_avg parameter: \n",
    "time_avg = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_th, sigma_2, Theta, wtheta, wwtheta, rdstr, transport = make_variables(path, selected_files, time_avg)\n",
    "\n",
    "z = ds_stat.variables['z'][:]\n",
    "zh = ds_stat.variables['zh'][:]\n",
    "t = ds_stat.variables['time'][:]\n",
    "dTheta_dz = np.gradient(Theta, z, axis = 1)\n",
    "dwwtheta_dz = np.gradient(wwtheta, zh, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity (order of magnitude) checks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dTheta_dz\", dTheta_dz.shape, np.mean(dTheta_dz), np.std(dTheta_dz))\n",
    "print(\"sigma_th\", sigma_th.shape, np.mean(sigma_th), np.std(sigma_th))\n",
    "print(\"sigma_2\", sigma_2.shape, np.mean(sigma_2), np.std(sigma_2))\n",
    "print(\"Theta\", Theta.shape, np.mean(Theta), np.std(Theta))\n",
    "print(\"wtheta\", wtheta.shape, np.mean(wtheta), np.std(wtheta))\n",
    "print(\"wwtheta\", wwtheta.shape, np.mean(wwtheta), np.std(wwtheta))\n",
    "print(\"dwwtheta_dz\", dwwtheta_dz.shape, np.mean(dwwtheta_dz), np.std(dwwtheta_dz))\n",
    "print(\"rdtsr\", rdstr.shape, np.mean(rdstr), np.std(rdstr))\n",
    "print(\"transport\", transport.shape, np.mean(transport), np.std(transport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wwtheta[1,0:3])\n",
    "print((wwtheta[1,0]+wwtheta[1,1])/2, (wwtheta[1,1]+wwtheta[1,2])/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables that have a second dimension of 385 are calculated at zh values of 0, 6, 12, and so on. Variables with 384 are staggered and calculated in between there, at values of 3, 9, 15, and so on. We reshape the 385's to be 384's by taking the average of each consecutive pair. \n",
    "\n",
    "We test if this works above and below with old and new values of `wwtheta` (just a sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_2 = reshape_variables(sigma_2)\n",
    "wtheta = reshape_variables(wtheta)\n",
    "wwtheta = reshape_variables(wwtheta)\n",
    "dwwtheta_dz = reshape_variables(dwwtheta_dz)\n",
    "rdstr = reshape_variables(rdstr)\n",
    "transport = reshape_variables(transport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dTheta_dz\", dTheta_dz.shape, np.mean(dTheta_dz), np.std(dTheta_dz))\n",
    "print(\"sigma_th\", sigma_th.shape, np.mean(sigma_th), np.std(sigma_th))\n",
    "print(\"sigma_2\", sigma_2.shape, np.mean(sigma_2), np.std(sigma_2))\n",
    "print(\"Theta\", Theta.shape, np.mean(Theta), np.std(Theta))\n",
    "print(\"wtheta\", wtheta.shape, np.mean(wtheta), np.std(wtheta))\n",
    "print(\"wwtheta\", wwtheta.shape, np.mean(wwtheta), np.std(wwtheta))\n",
    "print(\"dwwtheta_dz\", dwwtheta_dz.shape, np.mean(dwwtheta_dz), np.std(dwwtheta_dz))\n",
    "print(wwtheta[1,0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot all variables just to make sure they look correct: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeStep = 6\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "\n",
    "# Plot potentialTemp\n",
    "plt.subplot(1, 3, 1)\n",
    "#plt.plot(Theta[timeStep], np.arange(0, len(Theta[timeStep,:])))\n",
    "plt.plot(Theta[timeStep], z)\n",
    "plt.title('Theta(th)')\n",
    "# plt.xlabel('Value')\n",
    "plt.ylabel('Depth')\n",
    "\n",
    "# Plot th_diff\n",
    "plt.subplot(1, 3, 2)\n",
    "# plt.plot(dTheta_dz[timeStep], np.arange(0, len(dTheta_dz[timeStep,:])))\n",
    "plt.plot(dTheta_dz[timeStep], z)\n",
    "plt.title('dTheta_dz')\n",
    "plt.xlabel('Value')\n",
    "# plt.ylabel('Depth')\n",
    "\n",
    "# Plot th_flux\n",
    "plt.subplot(1, 3, 3)\n",
    "# plt.plot(wtheta[timeStep], np.arange(0, len(wtheta[timeStep,:])))\n",
    "plt.plot(wtheta[timeStep], z)\n",
    "plt.title('wtheta (th_flux)')\n",
    "# plt.xlabel('Value')\n",
    "# plt.ylabel('Depth')\n",
    "\n",
    "\n",
    "plt.figure(figsize = (12, 6))\n",
    "\n",
    "# Plot potentialTemp\n",
    "plt.subplot(1, 4, 1)\n",
    "# plt.plot(sigma_th[timeStep], np.arange(0, len(sigma_th[timeStep,:])))\n",
    "plt.plot(sigma_th[timeStep], z)\n",
    "plt.title('theta2bar (sigma_th)')\n",
    "# plt.xlabel('Value')\n",
    "plt.ylabel('Depth')\n",
    "\n",
    "# Plot th_diff\n",
    "plt.subplot(1, 4, 2)\n",
    "# plt.plot(sigma_2[timeStep], np.arange(0, len(sigma_2[timeStep,:])))\n",
    "plt.plot(sigma_2[timeStep], z)\n",
    "plt.title('sigma_w^2 (sigma_2)')\n",
    "plt.xlabel('Value')\n",
    "# plt.ylabel('Depth')\n",
    "\n",
    "# Plot th_flux\n",
    "plt.subplot(1, 4, 3)\n",
    "# plt.plot(wwtheta[timeStep], np.arange(0, len(wwtheta[timeStep,:])))\n",
    "plt.plot(wwtheta[timeStep], z)\n",
    "plt.title('(wwtheta)')\n",
    "# plt.xlabel('Value')\n",
    "# plt.ylabel('Depth')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "# plt.plot(dwwtheta_dz[timeStep], np.arange(0, len(dwwtheta_dz[timeStep,:])))\n",
    "plt.plot(dwwtheta_dz[timeStep], z)\n",
    "plt.title('(dwwtheta_dz)')\n",
    "# plt.xlabel('Value')\n",
    "# plt.ylabel('Depth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make our constants, and sanity check them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtheta_surface, pbl_height, wstar, theta_star, scaling, ustar, grr, T_0, beta, ug, q = make_constants(path, selected_files, time_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"wtheta_surface\", wtheta_surface.shape, np.mean(wtheta_surface), np.std(wtheta_surface))\n",
    "print(\"pbl_height\", pbl_height.shape, np.mean(pbl_height), np.std(pbl_height))\n",
    "print(\"wstar\", wstar.shape, np.mean(wstar), np.std(wstar))\n",
    "print(\"theta_star\", theta_star.shape, np.mean(theta_star), np.std(theta_star))\n",
    "print(\"scaling\", scaling.shape, np.mean(scaling), np.std(scaling))\n",
    "print(\"ustar\", ustar.shape, np.mean(ustar), np.std(ustar))\n",
    "print(\"grr\", grr)\n",
    "print(\"T_0\", T_0)\n",
    "print(\"beta\", beta)\n",
    "print(\"ug\", ug.shape, np.mean(ug), np.std(ug))\n",
    "print(\"q\", q.shape, np.mean(q), np.std(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing P\n",
    "\n",
    "We want to compute the $P$ term. \n",
    "\n",
    "In the paper, Equation (11) for the heat flux budget in the convective boundary layer is: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\overline{w \\theta}}{\\partial t}=0=\\underbrace{-\\sigma_w^2 \\frac{\\mathrm{d} \\Theta}{\\mathrm{d} z}}_M \\underbrace{-\\frac{\\mathrm{d} \\overline{w w \\theta}}{\\mathrm{d} z}}_T \\underbrace{-\\frac{1}{\\rho_0} \\theta \\frac{\\mathrm{d} p}{\\mathrm{~d} z}}_P \\underbrace{+\\beta \\overline{\\theta^2}}_B\n",
    "$$\n",
    "\n",
    "Production terms: Mean gradient production ($M$), and bouyancy production ($B$). Destruction terms: turbulent flux transport ($T$) and the the pressure gradientâ€“potential temperature covariance ($P$). Here, **P can be computed as a residual**. \n",
    "\n",
    "$P$ acts as a destruction/sink term for the heat flux $\\overline{w \\theta}$. Equation (12) provides a parametrization for $P$ as: \n",
    "\n",
    "$$\n",
    "P=-\\frac{1}{\\rho_0} \\overline{\\theta \\frac{\\mathrm{d} p}{\\mathrm{~d} z}}=-C_1 \\frac{\\overline{w \\theta}}{\\tau_1} - C_2 \\beta \\overline{\\theta^2} + C_3 \\sigma_w^2 \\frac{\\mathrm{d} \\Theta}{\\mathrm{d} z}\n",
    "$$\n",
    "\n",
    "The first term $-C_1 \\frac{\\overline{w \\theta}}{\\tau_1}$ is the \"slow\" term. It is Rotta's return to isotropy part. Is inversely proportional to time scale $\\tau_1$. I think it may also be called the \"turbulence-turbulence\" interaction. The last two terms are the \"rapid\" part, with $C_2 \\beta \\overline{\\theta^2}$ being the bouyancy term, and $C_3 \\sigma_w^2 \\frac{\\mathrm{d} \\Theta}{\\mathrm{d} z}$ being the shear-turbulence term. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "We compute $P$ as a residual, and then see if we can reconstruct it using the variables from the parametrization ($\\overline{w \\theta}, \\overline{\\theta^2}, \\sigma_w^2, \\frac{\\mathrm{d} \\Theta}{\\mathrm{d} z}$).\n",
    "\n",
    "If the residual looks different from the direct compute, this means the constants ($C_1, C_2, C_3$) probably need some modification. We may add in other variables such as Wind `ug`, Surface heating `q`, `wstar`, and `ustar` along with others later. \n",
    "\n",
    "Computing `P` as residual using Eq 11: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing as residual \n",
    "M = (- sigma_2 * dTheta_dz)\n",
    "T = - dwwtheta_dz\n",
    "B = (beta * sigma_th)\n",
    "\n",
    "P = - M - T - B\n",
    "print(P.shape)\n",
    "print(f\"{np.mean(B):.7f}, {np.std(B):.7f}\")\n",
    "print(f\"{np.mean(M):.7f}, {np.std(M):.7f}\")\n",
    "print(f\"{np.mean(T):.7f}, {np.std(T):.7f}\")\n",
    "print(f\"{np.mean(P):.7f}, {np.std(P):.7f}\")\n",
    "max(P[6])/min(P[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also computing `P` directly using Eq 12. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau1 = 600 #unsure about this one at the moment, it is in seconds so this is ten minutes. maybe this is reasonable. \n",
    "\n",
    "\n",
    "#ADD TAU RIGHT HERE \n",
    "tau = pbl_height/wstar #length scale/velocity scale\n",
    "\n",
    "C1 = 3\n",
    "C2 = 5/12\n",
    "C3 = 2/5\n",
    "P_direct = - (C1 * wtheta/tau) - (C2 * beta * sigma_th) + (C3 * sigma_2 * dTheta_dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the two compare: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeStep = 6\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot P_direct\n",
    "plt.plot(P_direct[timeStep] * pbl_height[timeStep] / (wstar[timeStep] * theta_star[timeStep]), z, label='P (direct from eq 12)', color = \"firebrick\")\n",
    "\n",
    "# Plot P residual\n",
    "plt.plot(P[timeStep] * pbl_height[timeStep] / (wstar[timeStep] * theta_star[timeStep]), z, label='P (residual from eq 11)', color = \"black\")\n",
    "\n",
    "plt.ylim(0, 2000)\n",
    "plt.title('Comparison of P values')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Depth')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Changing the C's can get the directly computed P to look much more like the residual P. If we assume the residual is correct, this means these coefficients could use some work! C2 and C3 seem to have the most effect, with C1 not doing much except changing the shape of the down in the lower ABL. C2 and C3 can really get it to jump out at the inversion layer like the residual. \n",
    "\n",
    "A more detailed plot: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeStep = 10\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(P[timeStep], z, label = 'P (residual, eq 11)', color = \"black\")\n",
    "plt.plot(-0.004997506 * wtheta[timeStep], z, label = r'$- C_1 * \\overline{w\\theta} / \\tau$', color = \"dodgerblue\")\n",
    "plt.plot(-0.019068547 * sigma_th[timeStep], z, label = r'$- C_2 * \\beta * \\overline{\\theta^2}$', color = \"lightseagreen\")\n",
    "plt.plot(sigma_2[timeStep] * dTheta_dz[timeStep] * 2/5, z, label = r'$\\sigma_w^2 * \\frac{d\\Theta}{dz}$', color = \"mediumslateblue\")\n",
    "\n",
    "plt.ylim(0, 2000)\n",
    "plt.title('The ingredients of P')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Depth')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equation Discovery\n",
    "\n",
    "Let's begin setting up a model to compute `P` from those variables. We will use all moments in time, almost all of our simulations, and most of the ABL vertically. Smaller experiments were tried in our earlier notebook `Steps_to_EQ-Rotta_Sikorski.ipynb` and did not produce nearly as good of results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.s_[:, 0:200]\n",
    "\n",
    "y = P[indices].ravel()\n",
    "\n",
    "x0 = wtheta[indices].ravel()\n",
    "x1 = sigma_th[indices].ravel()\n",
    "x2 = sigma_2[indices].ravel()\n",
    "x3 = dTheta_dz[indices].ravel()\n",
    "X = np.column_stack([x0, x1, x2, x3])\n",
    "\n",
    "df_X = pd.DataFrame(X, columns=['wtheta', 'sigma_th', 'sigma_2', 'dTheta_dz'])\n",
    "df_y = pd.DataFrame(y, columns=['P'])\n",
    "print(df_X.shape, df_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PySRRegressor(\n",
    "    niterations=2500,  # increase me for better results\n",
    "    maxsize = 20, # allowing for appropriate complexity (x + y has size 3)\n",
    "    maxdepth = 4, # avoiding deep nesting\n",
    "    progress = False, # makes the printout less hectic in Jupyter\n",
    "    binary_operators=[\"+\", \"*\", \"-\", \"/\"],\n",
    "    # unary_operators=[\n",
    "    #     \"cos\",\n",
    "    #     \"exp\",\n",
    "    #     \"sin\",\n",
    "    #     \"inv(x) = 1/x\",\n",
    "    #     \"square\",\n",
    "    #     \"cube\",\n",
    "    #     # ^ Custom operator (julia syntax)\n",
    "    # ],\n",
    "    # extra_sympy_mappings={\"inv\": lambda x: 1 / x},\n",
    "    # ^ Define operator for SymPy as well\n",
    "    elementwise_loss=\"loss(prediction, target) = (prediction - target)^2\",\n",
    "    # ^ Custom loss function (julia syntax)\n",
    "    batching = True,\n",
    "    batch_size = 250,\n",
    "    complexity_of_operators={\"*\": 1, \"+\": 1, \"-\": 1,\n",
    "                             \"exp\": 3, \"sin\": 3, \"cos\": 3, \n",
    "                             \"inv\": 3, \"square\": 3, \"cube\": 3},\n",
    "    # ^ Custom complexity of particular operators\n",
    ")\n",
    "\n",
    "# add max depth and max size options\n",
    "# use a custom loss function? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(df_X,df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equations = model.equations_\n",
    "print(equations.shape)\n",
    "display(equations)\n",
    "print(\"Final Equation: \")\n",
    "display(equations.iloc[-1, 4])\n",
    "print(equations.iloc[-1,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some past discovered eq's: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discovered_eq1 = (-0.004997506 * wtheta) - (sigma_th * 0.019068547)\n",
    "discovered_eq2 = (((sigma_th * wtheta) + (sigma_2 * 0.4073766)) * dTheta_dz) - (0.02337299 * (sigma_th + (0.13268535 * wtheta)))\n",
    "# discovered_eq3 = (dTheta_dz * sigma_2 * ((0.61661136 - wtheta))) - 0.0271534 * sigma_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EQUATION 1\n",
    "# discovered value in front of sigma_th is -0.019068547 or - 0.02337299\n",
    "print(- C2*beta)\n",
    "#close enough\\\n",
    "\n",
    "#discovered value in front of wtheta is -0.004997506\n",
    "print(- C1/tau1)\n",
    "# also close enough\n",
    "\n",
    "\n",
    "#EQUATION 2\n",
    "#as far as eq2 goes, it appears that the added multiplication is not of an important size\n",
    "print(np.mean(abs(sigma_th[timeStep] * wtheta[timeStep] * dTheta_dz[timeStep])))\n",
    "print(np.mean(abs(dTheta_dz[timeStep] * sigma_2[timeStep] * 0.4)))\n",
    "\n",
    "8/12 * beta # so it appears C2 should be larger, maybe 8/12 instead of 5/12\n",
    "\n",
    "print(C1)\n",
    "print(3/tau1)\n",
    "print(2/tau1)\n",
    "print(3/1000)\n",
    "0.02337299 * 0.13268535\n",
    "\n",
    "# so either tau1 needs to be 1000 or C1 needs to be 2 instead of 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting to answer some questions (below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeStep = 10\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(P[timeStep], z, label = 'P (residual, eq 11)', color = \"black\")\n",
    "\n",
    "plt.plot(discovered_eq1[timeStep], z, label = 'EQ1 (no 3rd term)', color = 'forestgreen')\n",
    "plt.plot(discovered_eq2[timeStep], z, label = 'EQ2 (extra multiplies)', color = 'goldenrod')\n",
    "# plt.plot(discovered_eq3[timeStep], z, label = 'EQ3 (no 3rd term)', color = 'darkorange')\n",
    "\n",
    "plt.ylim(0, 2000)\n",
    "plt.title('Comparison of P values')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Depth')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y, model.predict(X))\n",
    "plt.xlabel('Truth')\n",
    "plt.ylabel('Prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try passing in the last term ($\\sigma_w^2 * \\frac{d\\Theta}{dz}$) as one, and see if that works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our new term (Multi)\n",
    "x4 = (sigma_2[indices] * dTheta_dz[indices]).ravel()\n",
    "\n",
    "X_mult = np.column_stack([x0, x1, x4])\n",
    "\n",
    "df_X_mult = pd.DataFrame(X_mult, columns=['wtheta', 'sigma_th', 'Multi'])\n",
    "df_y_mult = pd.DataFrame(y, columns=['P'])\n",
    "print(df_X_mult.shape, df_y_mult.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mult = PySRRegressor(\n",
    "    niterations=2500,  # increase me for better results\n",
    "    maxsize = 20, # allowing for appropriate complexity (x + y has size 3)\n",
    "    maxdepth = 4, # avoiding deep nesting\n",
    "    progress = False, # makes the printout less hectic in Jupyter\n",
    "    binary_operators=[\"+\", \"*\", \"-\", \"/\"],\n",
    "    unary_operators=[\n",
    "        \"cos\",\n",
    "        \"exp\",\n",
    "        \"sin\",\n",
    "        \"inv(x) = 1/x\",\n",
    "        \"square\",\n",
    "        \"cube\",\n",
    "        # ^ Custom operator (julia syntax)\n",
    "    ],\n",
    "    extra_sympy_mappings={\"inv\": lambda x: 1 / x},\n",
    "    # ^ Define operator for SymPy as well\n",
    "    elementwise_loss=\"loss(prediction, target) = (prediction - target)^2\",\n",
    "    # ^ Custom loss function (julia syntax)\n",
    "    batching = True,\n",
    "    batch_size = 250,\n",
    "    complexity_of_operators={\"*\": 1, \"+\": 1, \"-\": 1,\n",
    "                             \"exp\": 3, \"sin\": 3, \"cos\": 3, \n",
    "                             \"inv\": 3, \"square\": 3, \"cube\": 3},\n",
    "    # ^ Custom complexity of particular operators\n",
    ")\n",
    "\n",
    "# add max depth and max size options\n",
    "# use a custom loss function? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mult.fit(df_X_mult,df_y_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equations_mult = model_mult.equations_\n",
    "print(equations_mult.shape)\n",
    "display(equations_mult)\n",
    "print(\"Final Equation: \")\n",
    "display(equations_mult.iloc[-1, 4])\n",
    "print(equations_mult.iloc[-1,3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
